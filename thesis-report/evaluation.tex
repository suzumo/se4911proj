\chapter{Evaluation}\label{ch:eval}

The intent of this thesis was to create a neural network in Accelerate that is (1) sufficiently well-performing; (2) with good useability; and, (3) analyse the benefits and disadvantages of such implementation. I will go through each one in the sections below.

\section{Performance evaluation} \label{se:eval.performance}

In terms of performance, namely correctness, scaleability and relative speed, my implementation did not perform well as expected (see Chapter~\ref{ch:results}).

Although accuracy was on-par with MATLAB on training set (1), it had lower accuracy than other implementations, including MATLAB, on training set (2). According to~\cite{LeC98}, a two-layer fully connected neural network 300 hidden units for MNIST dataset should produce an error of approximately 4.7\%. Albeit the fact that LeCun (1998) uses a carefully tuned, SGD algorithm method named \textit{Stochastic Diagonal Levenberg-Marquardt}\footnote{This is a complex method, but amongst other techniques, LeCun (1998) calculates an individual learning rate for each weight before parsing the training set --- which is applied to the network around 20 times --- in a randomised order of \textit{patterns}, or the sorting of training samples in a properly randomised sequence.} to train his neural network~\ref{se:previmplem}, one should expect my Accelerate implementation to mirror the accuracy of MATLAB's performance as it did for training set (1). However,\ref{tb:acc.vs.cores} indicates that it is at average approximately 7.2\% less accurate than that of MATLAB. This seems to signify that the unresolved bugs and issues as previously mentioned in \ref{se:impl.limits} is a significant dampener on the correctness of my implementation.

Also worth noting is that the MATLAB implementation has a higher error rate of 8.3\% compared to LeCun's SGD neural network at 4.7\%. This seems to imply that BGD with conjugate gradient method may not be as effective as SGD (or perhaps, for this particular data set). 

Interstingly, my program with an error rate of 15.5\% is closer the error rate reported by a native Haskell neural network implementation called \texttt{neural}~\cite{Bru16}. Upon testing with MNIST training set, it produced an error rate of approximately 17\% after 50 iterations, taking 983.9 seconds with 8 cores. This program reports to have reached an accuracy of 90.05\% after 450 iterations. 

%Sadly, both of our accuracy rate is worse than the performance of LeCun's classic linear classifier\footnote{A linear classifier architecture is similar to a neural network without a hidden layer.} at 12\%.

In terms of scaleability, my implementation linearly increases the time taken compared to size of input with constant number of cores similar to MATLAB, and reaches a plateau rapidly upon adding more (see ~\ref{fig:traininggraphs}). We believe this may be due to the rolling and unrolling of weight vectors during \texttt{fmincg} operation as mentioned in \ref{se:impl.fmincg}. The amount of work involved in copying the large matrices probably adds a significant overhead, reducing the benefit of having multicores.

Such redundant work is also likely a factor in negatively affecting the speed, particularly for smaller data sets. For instance, the MATLAB program seems to perform much faster with the smaller training set (1) (see ~\ref{fig:traininggraphs}). With MNIST training set, however, my Accelerate program does perform more competitively --- but, further testing is required. 

I was unable to find \textit{speed} performance results for MNIST training set on neural networks with same architecture in other languages in order to do a relative speed performance apart from MATLAB\footnote{The closest one I could find was a C++ neural network\cite{Wol17}, but with only 30 hidden layers. This reduces the first weight vector from $235500$ to $23550$ and the second weight vector from $3010$ to $310$. This implementation also uses the faster SGD method in the manner of~\cite{LeC98} and also did not disclaim his testing environment. Training time taken is said to be 82 seconds, but there were too many unknown factors in this data to draw comparisons.}.

Yet, on a positive note, my Accelerate neural network finishes training at 122.3 seconds with 8 cores. This is despite the fact that this implementation is more or less a direct translation of the MATLAB code with minimal Accelerate optimisation. With further Accelerate naturalisation (and bug fixes), it may be possible to achieve a very reasonable speed performance!

\section{Usability and Accelerate} \label{se:eval.usability}

It is fairly reasonable to say that the ease and convenience of a programming language can affect a programmer's work. One of the main reasons in starting this thesis was to gauge the the ease of creating a neural network using Accelerate.

Firstly, Accelerate has a convenient syntax that Haskell-users will find easy to use. Although MATLAB may seem more convenient than Accelerate at times, especially in operations that requires array or matrix manipulations, MATLAB is also ambiguous and loose in its language, and meaning can get lost without more effort given by the reader (such as in scalar matrix multiplication). In contrast, Accelerate allows programmers to know exactly what is occuring with its grammar and also has many high performing libraries for array computations that are easy-to-use.

Secondly, Accelerate is succinct, but perhaps not as abbreviated as in MATLAB to become too obscure to its readers. Neither is Accelerate as verbose as C++, which can become cumbersome to read and write as seen in \ref{fig:eval.syntax.predict}.

\begin{figure}
  \begin{lstlisting}
	-- in MATLAB
	function p = predict(Theta1, Theta2, X)
	m = size(X, 1);
	h1 = sigmoid([ones(m,1) X] * Theta1');
	h2 = sigmoid([ones(m,1) h1] * Theta2');
	[~,p] = max(h2, [], 2);
	end
	  	
	-- in Accelerate  	
  	predict :: Acc (Matrix Float) -> Acc (Matrix Float) -> Acc (Matrix Float) -> Acc (Vector Int)
    predict theta1 theta2 xs = 
    let
        Z :. m :. n = unlift (shape h1) :: Z :. Exp Int :. Exp Int
        h1 = A.map sigmoid 
           $ xs <> A.transpose theta1                
        h2 = A.map sigmoid 
           $ ((fill (lift (Z:.m:.(constant 1))) 1 :: Acc (Matrix Float)) A.++ h1)
             <>
             (A.transpose theta2)

        getYs :: Acc (Vector Int)
        getYs
          = A.map ((+1) . A.indexHead . A.fst)
          $ A.fold1 (\x y -> A.snd x A.> A.snd y ? (x , y))
          $ A.indexed h2
    in
    getYs
  	
  	-- in C++
	uint8 ForwardPass (const float* pixels, uint8 correctLabel) {
        for (size_t neuronIndex = 0; neuronIndex < HIDDEN_NEURONS; ++neuronIndex) {
            float Z = m_hiddenLayerBiases[neuronIndex];
            for (size_t inputIndex = 0; inputIndex < INPUTS; ++inputIndex)
                Z += pixels[inputIndex] * m_hiddenLayerWeights[HiddenLayerWeightIndex(inputIndex, neuronIndex)];
            m_hiddenLayerOutputs[neuronIndex] = 1.0f / (1.0f + std::exp(-Z));
        }
 
        for (size_t neuronIndex = 0; neuronIndex < OUTPUT_NEURONS; ++neuronIndex) {
            float Z = m_outputLayerBiases[neuronIndex];
            for (size_t inputIndex = 0; inputIndex < HIDDEN_NEURONS; ++inputIndex)
                Z += m_hiddenLayerOutputs[inputIndex] * m_outputLayerWeights[OutputLayerWeightIndex(inputIndex, neuronIndex)];
            m_outputLayerOutputs[neuronIndex] = 1.0f / (1.0f + std::exp(-Z));
        }

        ...
        return maxLabel;
    }
  \end{lstlisting}
  \caption{Comparing \texttt{predict} in MATLAB~\cite{Ng12}, Accelerate and C++~\cite{Wol17}.}
  \label{fig:eval.syntax.predict}
\end{figure}

There are, however, several elements that can intimidate new users to Accelerate. For example, I found it was quite difficult to debug my program, as Accelerate computations are not observable until they are returned to Haskell world. 

Secondly, as previously mentioned in ~\ref{se:impl.fmincg}, Haskell's type inference could not automatically infer the types of some Accelerate variables, particularly in \texttt{unlift} operations. 

Other minor inconveniences include determining which situation called for \texttt{Exp} and \texttt{Acc} and why or when one should switch between those two data structures, the fact that it was not possible to extract values from \texttt{Exp}, and lastly, the initial set up was quite intimidating.

\section{Advantages of an Accelerate implementation} \label{se:eval.advantages}

There are several advantages to using Accelerate. First, it results in much simpler source programs as programs are written in Haskell syntax; Accelerate code is very similar to an ordinary Haskell code and there are minimal syntactic difference~\cite{Mar13}.

Second, as Accelerate is embedded in Haskell, it can benefit from inheriting Haskell's functional language characteristics. For instance, Haskell as a pure language is advantageous for parallel computations as it will prohibit side effects that can disrupt other threads.

Another Haskell characteristic is having a more powerful type system, which could enforce a stronger checking for certain properties --- thereby catching more errors --- at compile time. In comparison, languages like MATLAB is dynamically typed, meaning that the types may change during runtime. Futhermore, as a weakly typed programming language, its types can be implicitly converted whenever a mismatch occurs, making its program unpredictable and unreliable.

Thirdly, Accelerate uses a number of optimisation techniques to mitigate the overheads, such as array fusion and sharing recovery\cite{ChaKelLee11}. For instance, my implementation was very inefficient in the sense that it has generated repeating multiple copies of same variables due to many \texttt{while} loops. Accelerate will optimise the code to reduce the number of parameters to the bare minimum required during the production of the ASTs in compile time. It will also 'fuse' sequences of \textit{producer} operations, and \textit{producer/consumer} operations that creates needless intermediate arrays, overall resulting in a much cleaner, efficient program~\cite{McDChaGro}.

Finally, Accelerate is a dynamic code generator and as such it can, (1) optimise a program at the time of code generation simultaneously by obtaining information about the hardware capabilities; (2) customise the generated program to the input data at the time of code generation; and (3) allow host programs to ``generate embedded programs on-the-fly"~\cite{ChaKelLee11}.

On the other hand, the disadvantage of using Accelerate is the extra overheads, which may originate at runtime (such as runtime code generation) and/or at execution time (such as kernel loading and data transfer)~\cite{ChaKelLee11}. Some of the overheads however, such as dynamic kernel compilation overheads, may not be so problematic in heavily data- and compute-intensive programs, because the proportion to the total time taken by the program may become insignificant~\cite{ChaKelLee11}; and, neural networks certainly fit in such a category of programs. 

Indeed, even with the small data sets in this implementation, the results in \ref{ch:results} seem to support this: for training set (1) the overhead was probably disproportionately big, making Accelerate implementaion perform much worse compared to MATLAB's; however, Accelerate performed better than MATLAB for training set (2).

\section{Incomplete works} \label{se:eval.incomplete}

~\cite{Eve16} uses sigmoid function. This function is mathematically convenient, because its gradient, $\sigma'(x)$, is easy to calculate:
$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$
The sigmoid function suffers from the following two disadvantages and is now mostly unused~\cite{Kar16}:
\begin{enumerate}
\item When the unit's activation \textit{saturates} at either tail of 0 or 1, the gradient of the sigmoid function nears zero (refer to the sigmoid function graph in Fig. \ref{fig:activation-graphs}). Zero gradients effectively `kill' any signal flows through the neuron in forward- and back-propagation. Thus saturated neurons results in a ``network [that] will barely learn"~\cite{Kar16}. Accordingly, extra care must be taken not to initialise the weights with a large value.
\item The range of the sigmoid function is not centered around zero. Hence if the inputs are always positive, it could introduce an undesirable \textit{zig-zagging dynamics} in the gradient updates for weights, as the gradient on the weights will be either all positive or all negative during back-propagation. However, this is a minor inconvenience as its impact is automatically mitigated in a BGD by the time the weights are updated.
\end{enumerate}


I believe that Chapter ~\ref{ch:results} training set 2 results indicate that a decently well-performing neural network should be possible.


This was the expected behaviour --> First, the comparative training set prediction results from Accelerate and MATLAB using sample data (1) indicates that my Accelerate implementation does align with the MATLAB program and should be thus, fairly accurate (see \ref{se:res.performance}).

But what I got was --> even if prediction is accurate, it does not always apply to everything else

Second: talk about difficulties with developing with Accelerate -- debugging. Maths. Type checking.

I did not conduct testing the neural network with different activation functions as I was aiming to test the speed and load performance of my implementation and such activation functions should not have a drastic impact on the performance of the neural network.

-- a full comparative analysis of other languages' implementation difficulty, performance, GPU harnessibility

\section{Future works} \label{se:eval.future}



-- from thesis A about creating testing data

Another problem that I may encounter using such pre-made repositories is that there may be insufficient training samples in the provided data set to train my neural network. In machine learning, a concept called \textit{artificial data synthesis} may resolve this dilemma~\cite{Ng12}. 

Artificial data synthesis comprises of the following two methods to create \textit{synthetic data}: 
\begin{enumerate}
\item Create new \textit{labeled} samples by superimposing original sample with a suitable replacement. For example, an original image of an alphabet can be edited to a new sample by overlaying the existing letter with a new font type of the same letter.
\item Amplify the data set by introducing \textit{smart} distortion to original sample. For instance, edit an original image of an alphabet into multiple versions by introducing various wave-like distortions.
\end{enumerate}

Artificial data synthesis is generally more efficient than obtaining raw data and labeling them manually. In the second method, the distortion must be non-trivial and the resulting sample must lie within a naturally occurring variance -- it must be reasonably found in the real world~\cite{Ng12}.
