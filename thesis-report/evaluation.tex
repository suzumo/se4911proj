\chapter{Evaluation}\label{ch:eval}

The intent of this thesis was to create a neural network in Accelerate that is (1) sufficiently well-performing; (2) with good useability; and, (3) analyse the benefits and disadvantages of such implementation. I will go through each one in the sections below.

\section{Performance evaluation} \label{se:eval.performance}

In terms of performance, namely accuracy, scalability and relative speed, my implementation did not perform as well as expected, due to a number of implementation efforts (see Chapter~\ref{ch:results}).

Although accuracy was on-par with MATLAB on the first training set, it had lower accuracy than other implementations, including MATLAB, on the second training set. According to~\cite{LeC98}, a two-layer fully connected neural network with 300 hidden units for the MNIST dataset should produce an error of approximately 4.7\%. Although they use a carefully tuned, SGD algorithm method named \textit{Stochastic Diagonal Levenberg-Marquardt}\footnote{This is a complex method, but amongst other techniques, they calculate an individual learning rate for each weight before input of the training set --- which is applied to the network around 20 times --- in a randomised order of \textit{patterns}, or the sorting of training samples in a properly randomised sequence.} to train his neural network (see~\ref{se:previmplem}), one should expect my Accelerate implementation to at least mirror the accuracy of the MATLAB implementation upon which it was based -- as it did for the first training set. However, my results (see~\ref{tb:acc.vs.cores}) indicate that it is on average approximately 7.2\% less accurate than that of MATLAB. This seems to signify that the unresolved bugs and issues as previously mentioned in \ref{se:impl.limits} are a significant dampener on the accuracy of my implementation.

Also worth noting is that the MATLAB implementation has a higher error rate of 8.3\% compared to LeCun's SGD neural network at 4.7\%. This seems to imply that BGD with conjugate gradient method may not be as effective as SGD (or perhaps, for this particular data set). 

Interestingly, my implementation with an error rate of 15.5\% after 50 iterations is closer the error rate reported by a native Haskell neural network implementation called \texttt{neural}~\cite{Bru16}. Upon testing with MNIST training set, it produced an error rate of approximately 17\% after 50 iterations, taking 983.9 seconds with 8 cores. It reports to have reached an accuracy of 90.05\% after 450 iterations. 

%Sadly, both of our accuracy rate is worse than the performance of LeCun's classic linear classifier\footnote{A linear classifier architecture is similar to a neural network without a hidden layer.} at 12\%.

In terms of scalability, my implementation linearly increases the time taken compared to size of input with constant number of cores similar to MATLAB, and reaches a plateau rapidly upon adding more (see ~\ref{fig:traininggraphs}). This may be due to the large amount of sequential \texttt{awhile} loops used in the code, and further overheads may be introduced from the rolling and unrolling of weight vectors during the \texttt{fmincg} operation as mentioned in \ref{se:impl.fmincg}, if Accelerate's fusion optimiser isn't eliminating these redundant operations.

Such redundant work is also likely a factor in negatively affecting the speed, particularly for smaller data sets. For instance, the MATLAB program seems to perform much faster with the smaller training set (1) (see ~\ref{fig:traininggraphs}). With MNIST training set, however, my Accelerate program does perform more competitively --- but, further testing is required. 

I was unable to find \textit{speed} performance results for MNIST training set on neural networks with same architecture in other languages in order to do a relative speed performance apart from MATLAB\footnote{The closest one I could find was a C++ neural network\cite{Wol17}, but with only 30 hidden layers. This reduces the first weight vector from $235500$ to $23550$ and the second weight vector from $3010$ to $310$. This implementation also uses the faster SGD method in the manner of~\cite{LeC98} and also did not disclaim his testing environment. Training time taken is said to be 82 seconds, but there were too many unknown factors in this data to draw comparisons.}.

Yet, on a positive note, my Accelerate neural network finishes training at 122.3 seconds with 8 cores. This is despite the fact that this implementation is more or less a direct translation of the MATLAB code with minimal Accelerate optimisation. With further Accelerate naturalisation (and bug fixes), it may be possible to achieve a very reasonable speed performance!

\section{Useability and Accelerate} \label{se:eval.useability}

\section{Advantages of an Accelerate implementation} \label{se:eval.advantages}

Acclerate has inherent benefits.

MATLAB is dynamically typed, meaning that the types may change during runtime. Futhermore, it is described as a weakly typed programming language, in that types are implicitly converted whenever a mismatch occurs.

For instance, even if the code is 'inefficient' in the sense that it has repeating parameters, by having 'sharing recovery', Accelerate will reduce the number of parameters to the bare minimum during the production of the ASTs.

There are several advantages to using Accelerate. First, it results in much simpler source programs as programs are written in Haskell syntax; Accelerate code is very similar to an ordinary Haskell code~\cite{Mar13}. For instance, Fig. \ref{fig:dotp} shows a dot product function comparison in Haskell and in Accelerate by~\cite{McD13}. There are minimal syntactic difference; for example, in the input and output types (Haskell's version takes and gives Haskell arrays while Accelerate's \texttt{dotp} deals with only Accelerate arrays), and in that Haskell's \texttt{foldl} is a left-to-right traversal, whereas Accelerate's \texttt{fold} is neither left nor right as it occurs in parallel~\cite{McD13}.

\begin{figure}
  \begin{lstlisting}
    -- dot product in Haskell
    dotp :: [Float] -> [Float] -> Float
    dotp xs ys = foldl (+) 0 (zipWith (*) xs ys)
           
    -- dot product in Accelerate
    dotp :: Acc (Vector Float) -> Acc (Vector Float) -> Acc (Scalar Float)
    dotp xs ys = fold (+) 0 (zipWith (*) xs ys)
  \end{lstlisting}
  \caption{Haskell and Accelerate versions for dot product~\cite{McD13}.}
  \label{fig:dotp}
\end{figure}

Second, as Accelerate is embedded in Haskell, it can benefit from inheriting Haskell's functional language characteristics. For instance, Haskell as a pure language is advantageous for parallel computations as it will prohibit side effects that can disrupt other threads; and, GPUs particularly require an extremely tight control flow due to massive numbers of threads that are generated. Another Haskell characteristic is having a more powerful type system, which could enforce a stronger checking for certain properties -- thereby catching more errors -- at compile time. An example of this is the use of types \texttt{Acc} and \texttt{Exp}, which separates array from scalar computations and prevents many cases of nested data parallelism, which is unsupported in Accelerate.

Finally, Accelerate is a dynamic code generator~\cite{ChaKelLee11} and as such it can,
\begin{itemize}
\item Optimise a program at the time of code generation simultaneously by obtaining information about the hardware capabilities.
\item Customise the generated program to the input data at the time of code generation.
\item Allow host programs to ``generate embedded programs on-the-fly".
\end{itemize}

On the other hand, the disadvantages of using Accelerate over CUDA are the extra overheads, which may originate at runtime (such as runtime code generation) and/or at execution time (such as kernel loading and data transfer)~\cite{ChaKelLee11}. 

Some of the overheads however, such as dynamic kernel compilation overheads, may not be so problematic in heavily data- and compute-intensive programs, because the proportion to the total time taken by the program may become insignificant~\cite{ChaKelLee11}; and, neural networks certainly fit in such a category of programs. Accelerate also uses a number of other techniques to mitigate the overheads, such as fusion, sharing recovery, caching and memoisation of compiled GPU kernels~\cite{ChaKelLee11}.

In terms of performance, Accelerate can be competitive with CUDA depending on the nature of the data input and the program~\cite{McDChaKel13}. 





\section{Incomplete works} \label{se:eval.incomplete}

~\cite{Eve16} uses sigmoid function. This function is mathematically convenient, because its gradient, $\sigma'(x)$, is easy to calculate:
$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$
The sigmoid function suffers from the following two disadvantages and is now mostly unused~\cite{Kar16}:
\begin{enumerate}
\item When the unit's activation \textit{saturates} at either tail of 0 or 1, the gradient of the sigmoid function nears zero (refer to the sigmoid function graph in Fig. \ref{fig:activation-graphs}). Zero gradients effectively `kill' any signal flows through the neuron in forward- and back-propagation. Thus saturated neurons results in a ``network [that] will barely learn"~\cite{Kar16}. Accordingly, extra care must be taken not to initialise the weights with a large value.
\item The range of the sigmoid function is not centered around zero. Hence if the inputs are always positive, it could introduce an undesirable \textit{zig-zagging dynamics} in the gradient updates for weights, as the gradient on the weights will be either all positive or all negative during back-propagation. However, this is a minor inconvenience as its impact is automatically mitigated in a BGD by the time the weights are updated.
\end{enumerate}


I believe that Chapter ~\ref{ch:results} training set 2 results indicate that a decently well-performing neural network should be possible.


This was the expected behaviour --> First, the comparative training set prediction results from Accelerate and MATLAB using sample data (1) indicates that my Accelerate implementation does align with the MATLAB program and should be thus, fairly accurate (see \ref{se:res.performance}).

But what I got was --> even if prediction is accurate, it does not always apply to everything else

Second: talk about difficulties with developing with Accelerate -- debugging. Maths. Type checking.

I did not conduct testing the neural network with different activation functions as I was aiming to test the speed and load performance of my implementation and such activation functions should not have a drastic impact on the performance of the neural network.

-- a full comparative analysis of other languages' implementation difficulty, performance, GPU harnessibility

\section{Future works} \label{se:eval.future}



-- from thesis A about creating testing data

Another problem that I may encounter using such pre-made repositories is that there may be insufficient training samples in the provided data set to train my neural network. In machine learning, a concept called \textit{artificial data synthesis} may resolve this dilemma~\cite{Ng12}. 

Artificial data synthesis comprises of the following two methods to create \textit{synthetic data}: 
\begin{enumerate}
\item Create new \textit{labeled} samples by superimposing original sample with a suitable replacement. For example, an original image of an alphabet can be edited to a new sample by overlaying the existing letter with a new font type of the same letter.
\item Amplify the data set by introducing \textit{smart} distortion to original sample. For instance, edit an original image of an alphabet into multiple versions by introducing various wave-like distortions.
\end{enumerate}

Artificial data synthesis is generally more efficient than obtaining raw data and labeling them manually. In the second method, the distortion must be non-trivial and the resulting sample must lie within a naturally occurring variance -- it must be reasonably found in the real world~\cite{Ng12}.
