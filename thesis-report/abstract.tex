\chapter*{Abstract}\label{abstract}

Parallel programming, including GPU programming, is commonly used to speed up neural network computations. However, the main languages for GPU programming are CUDA and OpenCL, which are very low-level languages that are cumbersome to use. This thesis includes the implementation of a feed-forward back-propagation neural network using Accelerate. Accelerate is an Embedded Domain-Specific Language in Haskell for high performance computing and has several accessibility advantages over CUDA, while still offering competitive performance on both GPUs and CPUs using LLVM. I assess its performance, benefits and disadvantages against the traditional approaches with CUDA and other languages. Future work includes the implementation of more sophisticated networks such as convolutional neural networks on top of this foundation.
