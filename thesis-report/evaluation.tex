\chapter{Evaluation}\label{ch:eval}

The intent of this thesis was to evaluate Accelerate's suitability in creating a neural network that is (1) sufficiently well-performing; (2) with good useability; and, (3) analyse the benefits and disadvantages of such implementation. I will go through each one in the sections below.

\section{Performance evaluation} \label{se:eval.performance}

I believe that Chapter ~\ref{ch:results} training set 2 results indicate that a decently well-performing neural network should be possible.

Current implementation has an issue with accuracy: according to~\cite{LeC98}, a two-layer fully connected neural network 300 hidden units for MNIST dataset should produce an error of approximately 4.7\%. According to the paper, LeCun uses a carefully tuned, SGD algorithm to train the neural network~\ref{se:previmplem}. My implementation, on the other hand, uses BGD with conjugate gradient method. Perhaps due to the bugs in \texttt{fmincg} or otherwise, my implementation has a high error rate of approximately 15.5\%, which is closer to the error rate of LeCun's classic linear classifier at 12\%. A linear classifier architecture is similar to a neural network without a hidden layer.

In terms of speed, I have found a C++ neural network that tested on the MNIST dataset\cite{Wol17}, but with only 30 hidden layers. This reduces the number of first weight vector from $235500$ to $23550$ and second weight vector from $3010$ to $310$. This implementation also uses the faster SGD method following ~\cite{LeC98}. Althought the testing environment of ~\cite{Wol17} is unclear, the training period is said to be 82 seconds.

With 8 cores, my Accelerate implementation runs at 122.3 seconds. This is despite the fact that this implementation is more or less a direct translation of the MATLAB code with minimal Accelerate naturalisation, and the list of unresolved issues (see~\ref{se:impl.limits}). With further optimisation and bug fixes, it may be possible to achieve a very reasonable performance and accuracy. 


\section{Benefits of Accelerate implementation} \label{se:impl.benefits}

Acclerate has inherent benefits.

For instance, even if the code is 'inefficient' in the sense that it has repeating parameters, by having 'sharing recovery', Accelerate will reduce the number of parameters to the bare minimum during the production of the ASTs.

\section{Incomplete works} \label{se:eval.incomplete}

This was the expected behaviour --> First, the comparative training set prediction results from Accelerate and MATLAB using sample data (1) indicates that my Accelerate implementation does align with the MATLAB program and should be thus, fairly accurate (see \ref{se:res.performance}).

But what I got was --> even if prediction is accurate, it does not always apply to everything else

Second: talk about difficulties with developing with Accelerate -- debugging. Maths. Type checking.

I did not conduct testing the neural network with different activation functions as I was aiming to test the speed and load performance of my implementation and such activation functions should not have a drastic impact on the performance of the neural network.

-- a full comparative analysis of other languages' implementation difficulty, performance, GPU harnessibility

\section{Future works} \label{se:eval.future}



-- from thesis A about creating testing data

Another problem that I may encounter using such pre-made repositories is that there may be insufficient training samples in the provided data set to train my neural network. In machine learning, a concept called \textit{artificial data synthesis} may resolve this dilemma~\cite{Ng12}. 

Artificial data synthesis comprises of the following two methods to create \textit{synthetic data}: 
\begin{enumerate}
\item Create new \textit{labeled} samples by superimposing original sample with a suitable replacement. For example, an original image of an alphabet can be edited to a new sample by overlaying the existing letter with a new font type of the same letter.
\item Amplify the data set by introducing \textit{smart} distortion to original sample. For instance, edit an original image of an alphabet into multiple versions by introducing various wave-like distortions.
\end{enumerate}

Artificial data synthesis is generally more efficient than obtaining raw data and labeling them manually. In the second method, the distortion must be non-trivial and the resulting sample must lie within a naturally occurring variance -- it must be reasonably found in the real world~\cite{Ng12}.
