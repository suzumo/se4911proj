\chapter{Evaluation}\label{ch:eval}

The intent of this thesis was to create a neural network in Accelerate that is (1) sufficiently well-performing; (2) with good usability; and, (3) analyse the benefits and disadvantages of such implementation. I will go through each one in the sections below.

\section{Performance evaluation} \label{se:eval.performance}

In terms of performance, namely correctness, scalability and relative speed, my implementation did not perform well as expected (see Chapter~\ref{ch:results}).

Although accuracy was on-par with MATLAB on training set (1), it had lower accuracy than other implementations, including MATLAB, on the second training set. According to~\cite{LeC98}, a two-layer fully connected neural network 300 hidden units for MNIST dataset should produce an error of approximately 4.7\%. Albeit the fact that LeCun (1998) uses a carefully tuned, SGD algorithm method named \textit{Stochastic Diagonal Levenberg-Marquardt}\footnote{This is a complex method, but amongst other techniques, LeCun (1998) calculates an individual learning rate for each weight before parsing the training set --- which is applied to the network around 20 times --- in a randomised order of \textit{patterns}, or the sorting of training samples in a properly randomised sequence.} to train his neural network, one should expect my Accelerate implementation to mirror the accuracy of MATLAB's performance as it did for training set (1). However,\ref{tb:acc.vs.cores} indicates that it is approximately 7.2\% less accurate than that of MATLAB. This seems to signify that the unresolved bugs and issues as previously mentioned in \ref{se:impl.limits} is a significant dampener on the accuracy of my implementation.

Also worth noting is that the MATLAB implementation has a higher error rate of 8.3\% compared to LeCun's SGD neural network at 4.7\%. This seems to imply that BGD with conjugate gradient method may not be as effective as SGD (or perhaps, for this particular data set). 

Interestingly, my program with an error rate of 15.5\% is closer the error rate reported by a native Haskell neural network implementation called \texttt{neural}~\cite{Bru16}. Upon testing with MNIST training set, \texttt{neural} produced an error rate of approximately 17\% after 50 iterations, taking 983.9 seconds with 8 cores. This program reports it can ultimately reach an accuracy of 90.05\% after 450 iterations. 

%Sadly, both of our accuracy rate is worse than the performance of LeCun's classic linear classifier\footnote{A linear classifier architecture is similar to a neural network without a hidden layer.} at 12\%.

In terms of scalability, my implementation linearly increases the time taken compared to size of input with constant number of cores similar to MATLAB, and reaches a plateau rapidly upon adding more (see ~\ref{fig:traininggraphs}). We believe this may be due to the rolling and unrolling of weight vectors during \texttt{fmincg} operation as mentioned in \ref{se:impl.fmincg}. The amount of work involved in copying the large matrices probably adds a significant overhead, reducing the benefit of having multicores.

Such redundant work is also likely a factor in negatively affecting the speed, particularly for smaller data sets. For instance, the MATLAB program seems to perform much faster with the smaller training set (1) (see ~\ref{fig:traininggraphs}). With MNIST training set, however, my Accelerate program does perform more competitively --- but, further testing is required. 

I was unable to find \textit{speed} performance results for MNIST training set on neural networks with same architecture in other languages in order to do a relative speed performance apart from MATLAB\footnote{The closest one I could find was a C++ neural network\cite{Wol17}, but with only 30 hidden layers. This reduces the first weight vector from $235500$ to $23550$ and the second weight vector from $3010$ to $310$. This implementation also uses the faster SGD method in the manner of~\cite{LeC98} and also did not disclaim his testing environment. Training time taken is said to be 82 seconds, but there were too many unknown factors in this data to draw comparisons.}.

Yet, on a positive note, my Accelerate neural network finishes training at 122.3 seconds with 8 cores. This is despite the fact that this implementation is more or less a direct translation of the MATLAB code with minimal Accelerate optimisation. With further Accelerate naturalisation (and bug fixes), it may be possible to achieve a very reasonable speed performance!

\section{Usability and Accelerate} \label{se:eval.usability}

It is fairly reasonable to say that the ease and convenience of a programming language can affect a programmer's work. One of the main reasons in starting this thesis was to gauge the ease of creating a neural network using Accelerate.

Firstly, Accelerate has a convenient syntax that Haskell-users will find easy to use. Although MATLAB may seem more convenient than Accelerate at times, especially in operations that requires array or matrix manipulations, MATLAB is also ambiguous and loose in its language, and meanings can get lost without more effort being invested by the reader (see ~\ref{se:impl.matlab.nn}). In contrast, Accelerate is explicit and unambiguous in its syntax, with clear dynamic semantics and a static type system to catch errors. It also has a growing collection of high performance libraries for array computations that are easy-to-use.

Secondly, Accelerate is compact and succinct, but perhaps not as abbreviated as in MATLAB to become obscure to its users. Neither is Accelerate as verbose as C++, which can become cumbersome to read and write as seen in ~\ref{fig:eval.syntax.predict}.

\begin{figure}
  \begin{lstlisting}
	-- in MATLAB
	function p = predict(Theta1, Theta2, X)
	m = size(X, 1);
	h1 = sigmoid([ones(m,1) X] * Theta1');
	h2 = sigmoid([ones(m,1) h1] * Theta2');
	[~,p] = max(h2, [], 2);
	end
	  	
	-- in Accelerate  	
  	predict :: Acc (Matrix Float) -> Acc (Matrix Float) -> Acc (Matrix Float) -> Acc (Vector Int)
    predict theta1 theta2 xs = 
    let
        Z :. m :. n = unlift (shape h1) :: Z :. Exp Int :. Exp Int
        h1 = A.map sigmoid 
           $ xs <> A.transpose theta1                
        h2 = A.map sigmoid 
           $ ((fill (lift (Z:.m:.(constant 1))) 1 :: Acc (Matrix Float)) A.++ h1)
             <>
             (A.transpose theta2)

        getYs :: Acc (Vector Int)
        getYs
          = A.map ((+1) . A.indexHead . A.fst)
          $ A.fold1 (\x y -> A.snd x A.> A.snd y ? (x , y))
          $ A.indexed h2
    in
    getYs
  	
  	-- in C++
	uint8 ForwardPass (const float* pixels, uint8 correctLabel) {
        for (size_t neuronIndex = 0; neuronIndex < HIDDEN_NEURONS; ++neuronIndex) {
            float Z = m_hiddenLayerBiases[neuronIndex];
            for (size_t inputIndex = 0; inputIndex < INPUTS; ++inputIndex)
                Z += pixels[inputIndex] * m_hiddenLayerWeights[HiddenLayerWeightIndex(inputIndex, neuronIndex)];
            m_hiddenLayerOutputs[neuronIndex] = 1.0f / (1.0f + std::exp(-Z));
        }
        
        ... 
        return maxLabel;
    }
  \end{lstlisting}
  \caption{Comparing \texttt{predict} in MATLAB~\cite{Ng12}, Accelerate and C++~\cite{Wol17}.}
  \label{fig:eval.syntax.predict}
\end{figure}

There are, however, several elements that can intimidate new users to Accelerate. For example, I found it was quite difficult to debug my program, as Accelerate computations are not observable until they are returned to Haskell world. 

Secondly, as previously mentioned in ~\ref{se:impl.fmincg}, Haskell's type inference could not automatically infer the types of some Accelerate variables, particularly in \texttt{unlift} operations. 

Other minor inconveniences include determining which situation called for \texttt{Exp} and \texttt{Acc} and why or when one should switch between those two data structures, the fact that it was not possible to extract values from \texttt{Exp}, and lastly, the initial set up was quite intimidating.

\section{Advantages of an Accelerate implementation} \label{se:eval.advantages}

There are several advantages to using Accelerate. First, it results in much simpler source programs as programs are written in Haskell syntax; Accelerate code is very similar to an ordinary Haskell code and there are minimal syntactic difference~\cite{Mar13}.

Second, as Accelerate is embedded in Haskell, it can benefit from inheriting Haskell's functional language characteristics. For instance, Haskell as a pure language is advantageous for parallel computations as it will prohibit side effects that can disrupt other threads.

Another Haskell characteristic is having a more powerful type system, which could enforce a stronger checking for certain properties --- thereby catching more errors --- at compile time. In comparison, languages like MATLAB are dynamically typed, meaning that the types may change during runtime. Futhermore, as a weakly typed programming language, its types can be implicitly converted whenever a mismatch occurs, making its program unpredictable and unreliable.

Thirdly, Accelerate uses a number of optimisation techniques to mitigate the overheads, such as array fusion and sharing recovery\cite{ChaKelLee11}. For instance, my implementation was very inefficient in the sense that it would have generated multiple copies of same variables due to many \texttt{while} loops in a non-optimising compiler. Accelerate will optimise the code to reduce the number of parameters to the bare minimum required during the production of the ASTs in compile time. It will also 'fuse' sequences of \textit{producer/producer} operations and fuse \textit{producer/consumer} operations to eliminate intermediate arrays --- overall, resulting in a much cleaner, efficient program~\cite{McDChaGro}.

Finally, Accelerate is a dynamic code generator and as such it can, (1) optimise a program at the time of code generation simultaneously by obtaining information about the hardware capabilities; (2) customise the generated program to the input data at the time of code generation; and (3) allow host programs to ``generate embedded programs on-the-fly"~\cite{ChaKelLee11}.

On the other hand, the disadvantage of using Accelerate is the extra overheads, which may originate at runtime (such as runtime code generation) and/or at execution time (such as kernel loading and data transfer)~\cite{ChaKelLee11}. Some of the overheads however, such as dynamic kernel compilation overheads, may not be so problematic in heavily data- and compute-intensive programs, because the proportion to the total time taken by the program may become insignificant~\cite{ChaKelLee11}; and, neural networks certainly fit in such a category of programs. 

Indeed, even with the small data sets in this implementation, the results in Chapter \ref{ch:results} seem to support this: for the small training set (1) the overhead outweighed the parallelism advantages, making the Accelerate implementation perform more poorly compared to MATLAB; however, Accelerate performed better than MATLAB for the larger training set (2).

\section{Incomplete works} \label{se:eval.incomplete}

As stated in ~\ref{se:impl.limits}, there are a number of issues that requires further work in my current implementation. First is resolving the unexpected behaviour in the inner and middle loops when \texttt{M} variable is not set at 1. Secondly, the rolling and unrolling of weight matrices should be optimised or eliminated, in order to reduce overhead. 

I would also test the accuracy of the predictions by trying a different activation functions. I have used the sigmoid function in this implementation, initially in order to confirm that my development is in alignment with MATLAB's and also because it is mathematically convenient, because its gradient, $\sigma'(x)$, is easy to calculate:
$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$
However, sigmoid function is not widely used these days, due to these two disadvantages~\cite{Kar16}:
\begin{enumerate}
\item When the unit's activation \textit{saturates} at either tail of 0 or 1, the gradient of the sigmoid function nears zero (refer to the sigmoid function graph in Fig. \ref{fig:activation-graphs}). Zero gradients effectively `kill' any signal flows through the neuron in forward- and back-propagation. Thus saturated neurons results in a ``network [that] will barely learn"~\cite{Kar16}. Accordingly, extra care must be taken not to initialise the weights with a large value.
\item The range of the sigmoid function is not centered around zero. Hence if the inputs are always positive, it could introduce an undesirable \textit{zig-zagging dynamics} in the gradient updates for weights, as the gradient on the weights will be either all positive or all negative during back-propagation. However, this is a minor inconvenience as its impact is automatically mitigated in a BGD by the time the weights are updated.
\end{enumerate}

It would be also interesting to implement a SGD neural network to test for its relative performance.

Finally, after optimising the current implementation, there are many complicated neural network architectures that could be built by using this simple neural network as preliminary building blocks. For instance, convolutional neural networks, or \textit{convNets}, are one of the most reliable and efficient performers in image recognition problems. ConvNets work by processing only portions of the input image, which are tiled in subsequent layers in such a way that the input regions overlap and is deemed to obtain a better representation of the original image at a fraction of the computational cost ~\cite{Kar16}. Each of the tiles are a small simple neural network!
